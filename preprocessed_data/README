## Preparing preprocessed data (multi-GPU)

This file contains example `torchrun` commands to prepare text + latent data using `prepare_data_multi_gpu.py` (use this script instead of `prepare_data_distributed.py`). Run these from a zsh shell on macOS and ensure your CUDA/torchrun environment is configured.

Examples:

- With T5-large on 2 GPUs

```bash
torchrun --nnodes=1 --nproc_per_node=2 preprocessed_data/prepare_data_multi_gpu.py \
  --datasets openwebtext \
  --latent-model t5 \
  --batch-size 128 \
  	--max-samples 10000000 \
  --output-dir preprocessed_data/t5_1024d_full
```


- With SONAR on 2 GPUs

```bash
torchrun --nnodes=1 --nproc_per_node=2 preprocessed_data/prepare_data_multi_gpu.py \
	--datasets openwebtext \
	--latent-model sonar \
	--batch-size 128 \
	--max-samples 10000000 \
	--output-dir preprocessed_data/sonar_1024d_full
```

- With E5 (multilingual-e5-large) on 4 GPUs

```bash
torchrun --nnodes=1 --nproc_per_node=4 preprocessed_data/prepare_data_multi_gpu.py \
	--datasets openwebtext \
	--latent-model e5 \
	--batch-size 256 \
	--max-samples 10000000 \
	--output-dir preprocessed_data/e5_1024d_full
```

- With Qwen on 2 GPUs (careful with memory!)

```bash
torchrun --nnodes=1 --nproc_per_node=2 preprocessed_data/prepare_data_multi_gpu.py \
	--datasets openwebtext \
	--latent-model qwen \
	--batch-size 8 \
	--max-samples 10000000 \
	--output-dir preprocessed_data/qwen_1024d_full
```

Supported latent models:


- `sonar` (facebook/SONAR) — 1024-dim (used in our examples as 1024d)
- `e5` (intfloat/multilingual-e5-large) — 1024-dim (multilingual E5 large typically produces larger embeddings; example output uses 1024d)
- `bge` (BAAI/bge-m3) — BGE M3 embeddings (see snippet below); supports dense_vecs and long max_length.
- `t5` (google/t5-v1_1-large) — 1024-dim (hidden size from the T5-large encoder)
- `qwen` (Qwen/Qwen3-Embedding-0.6B) — 1024-dim (embedding model; lower memory than the large causal LM but still monitor batch size)

Notes:
- Adjust `--nproc_per_node` and `--batch-size` according to your GPU memory.
- `--max-samples 10000000` can be used to cap the number of samples processed per dataset (added above in examples).
- `torchrun` will set up the distributed environment; ensure `torch` + CUDA are installed and visible to each process.
- If you want a validation split, pass `--create-validation` and set `--val-ratio` as needed.


